A Log Query analyzer for PostgreSQL
=====================================
Point the command line tool to a single PostgreSQL csv log file or a log 
directory to import the contents and run analytics on the queries.

This tool allows you to display a table with the following filters:

* **Slowest:** Displays a block of queries ordered by slowest descending.
* **Usage:**   Displays the list of queries ordered by the amount of times
  used.
* **Weight:**  Basically (time to execute x times used)

Configuring PostgreSQL for query analisis
-----------------------------------------
One of the things needed before hand is to properly configure PostgreSQL to
output logs in CSV format so that the analizer can properly import data.

I know, weird - but it is a nice way to properly import the queries and parse
the things needed to display them later.

These are the configuration options with explaining comments on what they do

::

    #------------------------------------------------------------------------------
    # ERROR REPORTING AND LOGGING
    #------------------------------------------------------------------------------

    # - Where to Log -

    log_destination = 'csvlog'      # Valid values are combinations of
                        # stderr, csvlog, syslog, and eventlog,
                        # depending on platform.  csvlog
                        # requires logging_collector to be on.

    # This is used when logging to stderr:
    logging_collector = on      # Enable capturing of stderr and csvlog
                        # into log files. Required to be on for
                        # csvlogs.
                        # (change requires restart)

    # These are only used if logging_collector is on:
    log_directory = 'pg_log'        # directory where log files are written,
                        # can be absolute or relative to PGDATA

    # Modify this as necessary, the pattern does not matter for the importer
    log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' # log file name pattern,


Debug messages
--------------
This tool uses proper logging to output information to the terminal, you can
set the logging level desire by passing a value to ``-l`` or ``--logging-level``.
Acceptable values for that flag are:

* debug
* warning
* critical
* info

By default it will use ``INFO`` level.


Importing
---------
Either use the full path to a single CSV file from the output generated by
PostgreSQL or a directory. Both ``-i`` or ``--import`` should work ::

    guaman -i /path/to/postgresql/csv


Reporting
---------
The ``report`` command takes a few different arguments: ``all``, ``weight``,
``slowest`` and ``usage``.

As described in the main paragraph, it will output information in a descending
ordered list of queries **with** hashes that can later be used to show the
complete information gathered from a single query.

Single query reporting
----------------------
If there is a need to display all the information gathered by a single query,
this can be achieved by passing in the hash (always displayed when showing any
table) to the ``show`` command.

The SQL will get parsed and displaed nicely, in addition to that you should
expect to see the following information along with it:

* Full Hash
* Timestamp
* User
* Database
* Open (status of the connection)
* Status
* Error Code
* Error
* Duration (in seconds)
* Times ran
* Full Query

